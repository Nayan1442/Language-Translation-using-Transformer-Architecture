# ğŸŒ Language Translation using Transformer Architecture

A deep learning-based sequence-to-sequence model for automatic translation between languages using the Transformer architecture. This project demonstrates how attention mechanisms can be used to build efficient and scalable machine translation systems.

---

## ğŸ§  Overview

This project implements a Transformer-based Neural Machine Translation (NMT) system inspired by the seminal paper **"Attention Is All You Need"** by Vaswani et al. The model is trained to translate sentences from a **source language** (e.g., English) to a **target language** (e.g., French, German, Hindi, etc.).

---

## ğŸš€ Features

- âœ… Built with PyTorch
- âœ… Implements Encoder-Decoder Transformer architecture
- âœ… Uses Multi-Head Attention and Positional Encoding
- âœ… Supports teacher forcing during training
- âœ… Includes BLEU score evaluation
- âœ… Easily extendable to new language pairs

---

## ğŸ“‚ Project Structure

