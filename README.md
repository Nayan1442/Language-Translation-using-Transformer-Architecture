# 🌐 Language Translation using Transformer Architecture

A deep learning-based sequence-to-sequence model for automatic translation between languages using the Transformer architecture. This project demonstrates how attention mechanisms can be used to build efficient and scalable machine translation systems.

---

## 🧠 Overview

This project implements a Transformer-based Neural Machine Translation (NMT) system inspired by the seminal paper **"Attention Is All You Need"** by Vaswani et al. The model is trained to translate sentences from a **source language** (e.g., English) to a **target language** (e.g., French, German, Hindi, etc.).

---

## 🚀 Features

- ✅ Built with PyTorch
- ✅ Implements Encoder-Decoder Transformer architecture
- ✅ Uses Multi-Head Attention and Positional Encoding
- ✅ Supports teacher forcing during training
- ✅ Includes BLEU score evaluation
- ✅ Easily extendable to new language pairs

---

## 📂 Project Structure

